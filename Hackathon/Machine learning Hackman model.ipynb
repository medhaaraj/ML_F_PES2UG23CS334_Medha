{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L0QHRGJFCMqB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "START = \"<S>\"\n",
        "END = \"<E>\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training corpus\n",
        "with open(\"/content/corpus.txt\", \"r\") as f:\n",
        "    raw_words = f.read().splitlines()\n",
        "\n",
        "words = [w.strip().upper() for w in raw_words if w.strip()]\n",
        "words = [w for w in words if w.replace(\" \", \"\").isalpha()]\n",
        "\n",
        "print(f\"Loaded {len(words)} training words.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc6SWpguCVZ9",
        "outputId": "b29222bc-8a19-4bd8-b5dc-785fa397db38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 50000 training words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace normalization helper\n",
        "def normalize_with_laplace(counter, alpha=1.0, vocab=None):\n",
        "    if vocab is None:\n",
        "        vocab = list(counter.keys())\n",
        "    total = sum(counter.values()) + alpha * len(vocab)\n",
        "    return {k: (counter.get(k, 0) + alpha) / total for k in vocab}"
      ],
      "metadata": {
        "id": "8uAjB2MvCYiN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build tokenized words by length\n",
        "tokenized_by_length = defaultdict(list)\n",
        "for w in words:\n",
        "    tok = [START] + list(w) + [END]\n",
        "    tokenized_by_length[len(w)].append(tok)\n",
        "\n",
        "letters = [chr(c) for c in range(65, 91)]\n",
        "\n",
        "length_emission_probs = {}\n",
        "length_transition_probs = {}\n",
        "\n",
        "for L, toks in tokenized_by_length.items():\n",
        "    emiss = defaultdict(Counter)\n",
        "    trans = defaultdict(Counter)\n",
        "    for word in toks:\n",
        "        for i in range(1, len(word) - 1):\n",
        "            pos = i\n",
        "            prev = word[i - 1]\n",
        "            curr = word[i]\n",
        "            s = (pos, prev)\n",
        "            ns = (pos + 1, curr)\n",
        "            emiss[s][curr] += 1\n",
        "            trans[s][ns] += 1\n",
        "    length_emission_probs[L] = {s: normalize_with_laplace(c, 1.0, letters) for s, c in emiss.items()}\n",
        "    length_transition_probs[L] = {s: normalize_with_laplace(c, 1.0, list(c.keys())) for s, c in trans.items()}\n",
        "\n",
        "print(f\"Built HMMs for {len(length_emission_probs)} different word lengths.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxXVePb6CfRh",
        "outputId": "623c70eb-6c24-466d-b52c-755ff67f2494"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built HMMs for 24 different word lengths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_letter_probs_combined(masked_word, words_list, length_emission_probs, length_transition_probs, alpha_pattern_weight=2.0):\n",
        "    masked = masked_word.upper()\n",
        "    L = len(masked)\n",
        "    letters = list(masked)\n",
        "    letter_scores = Counter()\n",
        "\n",
        "    emission_probs = length_emission_probs.get(L, {})\n",
        "    transition_probs = length_transition_probs.get(L, {})\n",
        "\n",
        "    # HMM component\n",
        "    for i, ch in enumerate(letters):\n",
        "        if ch == \"_\":\n",
        "            prev_letter = letters[i - 1] if i > 0 else START\n",
        "            state = (i + 1, prev_letter)\n",
        "            if state in emission_probs:\n",
        "                for letter, prob in emission_probs[state].items():\n",
        "                    letter_scores[letter] += prob\n",
        "\n",
        "    # Pattern matching component\n",
        "    candidates = []\n",
        "    for w in words_list:\n",
        "        if len(w) != L:\n",
        "            continue\n",
        "        ok = True\n",
        "        for i, c in enumerate(masked):\n",
        "            if c != \"_\" and w[i] != c:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(w)\n",
        "\n",
        "    if candidates:\n",
        "        pattern_counts = Counter()\n",
        "        for w in candidates:\n",
        "            for i, c in enumerate(w):\n",
        "                if masked[i] == \"_\":\n",
        "                    pattern_counts[c] += 1\n",
        "        total = sum(pattern_counts.values())\n",
        "        for ch, cnt in pattern_counts.items():\n",
        "            weight = alpha_pattern_weight * (1.0 + (1.0 / max(1, len(candidates))))\n",
        "            letter_scores[ch] += (cnt / total) * weight\n",
        "\n",
        "    if not letter_scores:\n",
        "        letter_scores = {chr(c): 1.0 for c in range(65, 91)}\n",
        "\n",
        "    total = sum(letter_scores.values())\n",
        "    return {ch: letter_scores[ch] / total for ch in letter_scores}"
      ],
      "metadata": {
        "id": "sKqKGUTOClLp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_state_features(masked_word, guessed, lives):\n",
        "    \"\"\"Extract features from current game state for RL\"\"\"\n",
        "    features = []\n",
        "\n",
        "    # Basic features\n",
        "    word_length = len(masked_word)\n",
        "    num_revealed = sum(1 for c in masked_word if c != \"_\")\n",
        "    num_hidden = masked_word.count(\"_\")\n",
        "    num_guessed = len(guessed)\n",
        "    lives_remaining = lives\n",
        "\n",
        "    # Positional features\n",
        "    revealed_positions = [i for i, c in enumerate(masked_word) if c != \"_\"]\n",
        "    hidden_positions = [i for i, c in enumerate(masked_word) if c == \"_\"]\n",
        "\n",
        "    # Pattern features\n",
        "    consecutive_hidden = 0\n",
        "    max_consecutive_hidden = 0\n",
        "    for c in masked_word:\n",
        "        if c == \"_\":\n",
        "            consecutive_hidden += 1\n",
        "            max_consecutive_hidden = max(max_consecutive_hidden, consecutive_hidden)\n",
        "        else:\n",
        "            consecutive_hidden = 0\n",
        "        # Normalized features\n",
        "    features = [\n",
        "        word_length / 20.0,  # normalize by max word length\n",
        "        num_revealed / word_length if word_length > 0 else 0,\n",
        "        num_hidden / word_length if word_length > 0 else 0,\n",
        "        num_guessed / 26.0,\n",
        "        lives_remaining / 6.0,\n",
        "        max_consecutive_hidden / word_length if word_length > 0 else 0,\n",
        "    ]\n",
        "\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "QT2quDrvC2uy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLHangmanAgent:\n",
        "    \"\"\"Q-Learning agent with function approximation\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, discount=0.95, epsilon=0.1):\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Q-value weights for each letter (linear function approximation)\n",
        "        self.weights = {chr(c): np.random.randn(6) * 0.01 for c in range(65, 91)}\n",
        "\n",
        "        # Experience replay buffer\n",
        "        self.replay_buffer = []\n",
        "        self.buffer_size = 10000\n",
        "\n",
        "    def get_q_value(self, state_features, letter):\n",
        "        \"\"\"Compute Q(s, a) = w^T * phi(s)\"\"\"\n",
        "        return np.dot(self.weights[letter], state_features)\n",
        "\n",
        "    def get_action_values(self, masked_word, guessed, lives, base_probs):\n",
        "        \"\"\"Get Q-values combined with HMM probabilities\"\"\"\n",
        "        state_features = extract_state_features(masked_word, guessed, lives)\n",
        "\n",
        "        action_values = {}\n",
        "        for letter in letters:\n",
        "            if letter not in guessed:\n",
        "                # Combine Q-value with base probability\n",
        "                q_val = self.get_q_value(state_features, letter)\n",
        "                base_prob = base_probs.get(letter, 0.001)\n",
        "                # Weighted combination\n",
        "                action_values[letter] = 0.7 * base_prob + 0.3 * (1.0 / (1.0 + np.exp(-q_val)))  # sigmoid\n",
        "\n",
        "        return action_values\n",
        "\n",
        "    def act(self, masked_word, guessed, lives):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        base_probs = get_letter_probs_combined(\n",
        "            masked_word, words, length_emission_probs, length_transition_probs\n",
        "        )\n",
        "\n",
        "        # Remove already guessed letters\n",
        "        for g in guessed:\n",
        "            base_probs[g] = 0.0\n",
        "\n",
        "        if sum(base_probs.values()) == 0:\n",
        "            base_probs = {ch: 1.0 for ch in letters if ch not in guessed}\n",
        "\n",
        "        # Epsilon-greedy exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            available = [ch for ch in letters if ch not in guessed]\n",
        "            return random.choice(available) if available else 'A'\n",
        "\n",
        "        # Get action values\n",
        "        action_values = self.get_action_values(masked_word, guessed, lives, base_probs)\n",
        "\n",
        "        if not action_values:\n",
        "            available = [ch for ch in letters if ch not in guessed]\n",
        "            return random.choice(available) if available else 'A'\n",
        "\n",
        "        return max(action_values, key=action_values.get)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update with function approximation\"\"\"\n",
        "        state_features = extract_state_features(state['masked'], state['guessed'], state['lives'])\n",
        "\n",
        "        # Current Q-value\n",
        "        current_q = self.get_q_value(state_features, action)\n",
        "\n",
        "        # Target Q-value\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            # Get best next action value\n",
        "            base_probs = get_letter_probs_combined(\n",
        "                next_state['masked'], words, length_emission_probs, length_transition_probs\n",
        "            )\n",
        "            next_action_values = self.get_action_values(\n",
        "                next_state['masked'], next_state['guessed'], next_state['lives'], base_probs\n",
        "            )\n",
        "\n",
        "            if next_action_values:\n",
        "                max_next_q = max(self.get_q_value(\n",
        "                    extract_state_features(next_state['masked'], next_state['guessed'], next_state['lives']),\n",
        "                    a\n",
        "                ) for a in next_action_values.keys())\n",
        "            else:\n",
        "                max_next_q = 0\n",
        "\n",
        "            target_q = reward + self.gamma * max_next_q\n",
        "\n",
        "        # Gradient update\n",
        "        td_error = target_q - current_q\n",
        "        self.weights[action] += self.lr * td_error * state_features\n",
        "\n",
        "    def store_experience(self, experience):\n",
        "        \"\"\"Store experience in replay buffer\"\"\"\n",
        "        self.replay_buffer.append(experience)\n",
        "        if len(self.replay_buffer) > self.buffer_size:\n",
        "            self.replay_buffer.pop(0)\n",
        "\n",
        "    def replay_experiences(self, batch_size=32):\n",
        "        \"\"\"Train on random batch from replay buffer\"\"\"\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.replay_buffer, batch_size)\n",
        "        for exp in batch:\n",
        "            self.update(exp['state'], exp['action'], exp['reward'],\n",
        "                       exp['next_state'], exp['done'])"
      ],
      "metadata": {
        "id": "2Y5LjepLDL8a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HangmanEnv:\n",
        "    def __init__(self, word_list, emission_probs, transition_probs, max_lives=6):\n",
        "        self.word_list = word_list\n",
        "        self.emission_probs = emission_probs\n",
        "        self.transition_probs = transition_probs\n",
        "        self.max_lives = max_lives\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.word = random.choice(self.word_list)\n",
        "        self.masked = [\"_\" for _ in self.word]\n",
        "        self.guessed = set()\n",
        "        self.lives = self.max_lives\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return {\n",
        "            'masked': \"\".join(self.masked),\n",
        "            'guessed': self.guessed.copy(),\n",
        "            'lives': self.lives\n",
        "        }\n",
        "\n",
        "    def step(self, letter):\n",
        "        letter = letter.upper()\n",
        "        self.guessed.add(letter)\n",
        "\n",
        "        if letter in self.word:\n",
        "            for i, ch in enumerate(self.word):\n",
        "                if ch == letter:\n",
        "                    self.masked[i] = letter\n",
        "            # Reward proportional to letters revealed\n",
        "            num_revealed = self.word.count(letter)\n",
        "            reward = num_revealed * 2  # Positive reward for correct guess\n",
        "        else:\n",
        "            self.lives -= 1\n",
        "            reward = -5  # Penalty for wrong guess\n",
        "\n",
        "        done = (self.lives == 0) or (\"_\" not in self.masked)\n",
        "\n",
        "        # Bonus for winning\n",
        "        if done and \"_\" not in self.masked:\n",
        "            reward += 20\n",
        "        # Penalty for losing\n",
        "        elif done and self.lives == 0:\n",
        "            reward -= 10\n",
        "\n",
        "        return self.get_state(), reward, done"
      ],
      "metadata": {
        "id": "VPnGrWVGDgqG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rl_agent(agent, train_words, num_episodes=5000, batch_size=32):\n",
        "    \"\"\"Train the RL agent\"\"\"\n",
        "    env = HangmanEnv(train_words, length_emission_probs, length_transition_probs)\n",
        "\n",
        "    episode_rewards = []\n",
        "    win_rates = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        # Decay epsilon\n",
        "        agent.epsilon = max(0.01, 0.1 * (0.995 ** episode))\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state['masked'], state['guessed'], state['lives'])\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Store experience\n",
        "            agent.store_experience({\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward,\n",
        "                'next_state': next_state,\n",
        "                'done': done\n",
        "            })\n",
        "\n",
        "            # Update Q-values\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        # Replay experiences\n",
        "        if episode % 10 == 0:\n",
        "            agent.replay_experiences(batch_size)\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        # Track progress\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            recent_rewards = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {recent_rewards:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return agent\n"
      ],
      "metadata": {
        "id": "VEczwzBbD23O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_test_set(agent, test_words, emission_probs, transition_probs, num_games=None, use_rl=True):\n",
        "    \"\"\"Evaluate agent on test set\"\"\"\n",
        "    if num_games is None:\n",
        "        num_games = len(test_words)\n",
        "    else:\n",
        "        num_games = min(num_games, len(test_words))\n",
        "\n",
        "    # Disable exploration during evaluation\n",
        "    original_epsilon = agent.epsilon if use_rl else 0\n",
        "    if use_rl:\n",
        "        agent.epsilon = 0.0\n",
        "\n",
        "    env = HangmanEnv(test_words[:num_games], emission_probs, transition_probs, max_lives=6)\n",
        "    total_wins, total_wrong, total_repeat = 0, 0, 0\n",
        "    failed_words = []\n",
        "\n",
        "    for i in range(num_games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        wrong_this_game = 0\n",
        "        repeat_this_game = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state['masked'], state['guessed'], state['lives'])\n",
        "\n",
        "            if action in state['guessed']:\n",
        "                repeat_this_game += 1\n",
        "\n",
        "            lives_before = state['lives']\n",
        "            state, _, done = env.step(action)\n",
        "\n",
        "            if state['lives'] < lives_before:\n",
        "                wrong_this_game += 1\n",
        "\n",
        "        won = \"_\" not in state['masked']\n",
        "        if won:\n",
        "            total_wins += 1\n",
        "        else:\n",
        "            failed_words.append(env.word)\n",
        "        total_wrong += wrong_this_game\n",
        "        total_repeat += repeat_this_game\n",
        "\n",
        "    # Restore original epsilon\n",
        "    if use_rl:\n",
        "        agent.epsilon = original_epsilon\n",
        "\n",
        "    success_rate = total_wins / num_games\n",
        "    final_score = (success_rate * num_games) - (total_wrong * 5) - (total_repeat * 2)\n",
        "\n",
        "    print(f\"\\n--- TEST SET RESULTS ---\")\n",
        "    print(f\"Games: {num_games}\")\n",
        "    print(f\"Wins: {total_wins}, Losses: {num_games - total_wins}\")\n",
        "    print(f\"Success rate: {success_rate:.2%}\")\n",
        "    print(f\"Wrong guesses: {total_wrong}, Repeats: {total_repeat}\")\n",
        "    print(f\"Final score: {final_score:.2f}\")\n",
        "\n",
        "    if failed_words:\n",
        "        print(f\"Some failed words: {failed_words[:10]}\")\n",
        "\n",
        "    return success_rate, total_wrong, total_repeat, final_score\n",
        "\n",
        "\n",
        "# ============ TRAINING AND EVALUATION ============\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING RL AGENT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create and train RL agent\n",
        "rl_agent = RLHangmanAgent(learning_rate=0.01, discount=0.95, epsilon=0.1)\n",
        "rl_agent = train_rl_agent(rl_agent, words, num_episodes=5000, batch_size=32)\n",
        "\n",
        "# Load test words\n",
        "with open(\"/content/test.txt\", \"r\") as f:\n",
        "    test_raw = f.read().splitlines()\n",
        "\n",
        "test_words = [w.strip().upper() for w in test_raw if w.strip()]\n",
        "test_words = [w for w in test_words if w.replace(\" \", \"\").isalpha()]\n",
        "print(f\"\\nLoaded {len(test_words)} test words.\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATING ON TEST SET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_result = evaluate_on_test_set(rl_agent, test_words, length_emission_probs,\n",
        "                                   length_transition_probs, num_games=2000)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"RL Agent Test: {test_result[0]*100:.2f}% success | Score: {test_result[3]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbxBz1E_ELXE",
        "outputId": "0fc11f61-36d0-43cd-9998-5363da72d5f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TRAINING RL AGENT\n",
            "==================================================\n",
            "Episode 500/5000 | Avg Reward: 25.41 | Epsilon: 0.010\n",
            "Episode 1000/5000 | Avg Reward: 27.66 | Epsilon: 0.010\n",
            "Episode 1500/5000 | Avg Reward: 28.25 | Epsilon: 0.010\n",
            "Episode 2000/5000 | Avg Reward: 21.42 | Epsilon: 0.010\n",
            "Episode 2500/5000 | Avg Reward: 23.70 | Epsilon: 0.010\n",
            "Episode 3000/5000 | Avg Reward: 24.54 | Epsilon: 0.010\n",
            "Episode 3500/5000 | Avg Reward: 28.72 | Epsilon: 0.010\n",
            "Episode 4000/5000 | Avg Reward: 25.11 | Epsilon: 0.010\n",
            "Episode 4500/5000 | Avg Reward: 23.90 | Epsilon: 0.010\n",
            "Episode 5000/5000 | Avg Reward: 25.54 | Epsilon: 0.010\n",
            "\n",
            "Loaded 2000 test words.\n",
            "\n",
            "==================================================\n",
            "EVALUATING ON TEST SET\n",
            "==================================================\n",
            "\n",
            "--- TEST SET RESULTS ---\n",
            "Games: 2000\n",
            "Wins: 643, Losses: 1357\n",
            "Success rate: 32.15%\n",
            "Wrong guesses: 10390, Repeats: 0\n",
            "Final score: -51307.00\n",
            "Some failed words: ['MEJORANA', 'CREEPMOUSY', 'EXAGGERATED', 'SESTI', 'FRECKLISH', 'IMMANACLE', 'UNFORWARDED', 'UNWARPABLE', 'SCHIZOIDISM', 'INCUDAL']\n",
            "==================================================\n",
            "RL Agent Test: 32.15% success | Score: -51307.00\n"
          ]
        }
      ]
    }
  ]
}